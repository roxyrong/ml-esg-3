{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook - Text Segmentations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:01:58.234793600Z",
     "start_time": "2024-01-23T13:01:56.192137200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:01.946822200Z",
     "start_time": "2024-01-23T13:02:01.066135300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_English = pd.read_parquet(\"../dataset/train_df_English_translated.parquet\")\n",
    "df_French = pd.read_parquet(\"../dataset/train_df_French_translated.parquet\")\n",
    "df_Korean = pd.read_parquet(\"../dataset/train_df_Korean_translated.parquet\")\n",
    "df_Chinese = pd.read_parquet(\"../dataset/train_df_Chinese_translated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:06.274822300Z",
     "start_time": "2024-01-23T13:02:06.238710400Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\"sentence\", \"Translation\", \"impact_length_idx\", \"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:08.387134400Z",
     "start_time": "2024-01-23T13:02:08.361034200Z"
    }
   },
   "outputs": [],
   "source": [
    "df_English[\"Translation\"] = df_English[\"sentence\"]\n",
    "df_English = df_English[columns]\n",
    "df_Chinese = df_Chinese[columns]\n",
    "df_Korean = df_Korean[columns]\n",
    "df_French = df_French[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:16.014928300Z",
     "start_time": "2024-01-23T13:02:15.266162100Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_English, df_Chinese, df_Korean, df_French]).reset_index(drop=True)\n",
    "df.to_parquet(\"../dataset/train_df_all_english.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:27.424371700Z",
     "start_time": "2024-01-23T13:02:27.404296300Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    return len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:27.783753Z",
     "start_time": "2024-01-23T13:02:27.729634300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_57428\\1269990977.py:2: FutureWarning: The provided callable <function mean at 0x000002677F991AF0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  df.groupby(\"language\").agg({\"wc\": np.mean})\n"
     ]
    },
    {
     "data": {
      "text/plain": "                  wc\nlanguage            \nChinese   903.184659\nEnglish    73.864220\nFrench     96.636914\nKorean    555.970000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wc</th>\n    </tr>\n    <tr>\n      <th>language</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Chinese</th>\n      <td>903.184659</td>\n    </tr>\n    <tr>\n      <th>English</th>\n      <td>73.864220</td>\n    </tr>\n    <tr>\n      <th>French</th>\n      <td>96.636914</td>\n    </tr>\n    <tr>\n      <th>Korean</th>\n      <td>555.970000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"wc\"] = df[\"Translation\"].map(word_count)\n",
    "df.groupby(\"language\").agg({\"wc\": np.mean})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korean Dataset Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:28.345623600Z",
     "start_time": "2024-01-23T13:02:28.330570500Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter very short sentences and non-sentences (defined by no . at the end of the sentence)\n",
    "def Chinese_article_preprocess(article):\n",
    "    sentences = article.split(\"\\n\")\n",
    "    filtered_sentences = [text for text in sentences if len(text.split(\" \")) > 5 ]\n",
    "    title = filtered_sentences[0]\n",
    "    filtered_sentences = filtered_sentences[1:]\n",
    "    filtered_sentences = [text for text in filtered_sentences if text[-1] == \".\"]\n",
    "    article = title + \" \" + \" \".join(filtered_sentences)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:29.317645Z",
     "start_time": "2024-01-23T13:02:29.290069100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Chinese[\"processed\"] = df_Chinese[\"Translation\"].map(Chinese_article_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:29.965352800Z",
     "start_time": "2024-01-23T13:02:29.919607100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.0795454545455\n"
     ]
    }
   ],
   "source": [
    "# we reduced the average word count from 900 to 750.\n",
    "df_Chinese[\"wc\"] = df_Chinese[\"processed\"].map(word_count)\n",
    "print(np.mean(df_Chinese[\"wc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:30.619542800Z",
     "start_time": "2024-01-23T13:02:30.471287600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Chinese['sentences'] = df_Chinese['processed'].apply(sent_tokenize)\n",
    "\n",
    "def group_sentences(sentences):\n",
    "    return [sentences[i:i+5] for i in range(0, len(sentences), 5)]\n",
    "\n",
    "df_Chinese['grouped_sentences'] = df_Chinese['sentences'].apply(group_sentences)\n",
    "new_df_Chinese = df_Chinese.explode('grouped_sentences')\n",
    "new_df_Chinese[\"grouped_sentences\"] = new_df_Chinese[\"grouped_sentences\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:30.789154200Z",
     "start_time": "2024-01-23T13:02:30.769178100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "1717\n"
     ]
    }
   ],
   "source": [
    "print(len(df_Chinese))\n",
    "print(len(new_df_Chinese))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:31.049673400Z",
     "start_time": "2024-01-23T13:02:31.009100800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.57483983692487\n"
     ]
    }
   ],
   "source": [
    "new_df_Chinese[\"wc\"] = new_df_Chinese[\"grouped_sentences\"].apply(word_count)\n",
    "print(np.mean(new_df_Chinese[\"wc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:31.330126Z",
     "start_time": "2024-01-23T13:02:31.286038800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "impact_length_idx\n0.0     337\n1.0     315\n2.0    1065\nName: grouped_sentences, dtype: int64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_Chinese.groupby(\"impact_length_idx\")[\"grouped_sentences\"].count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korean Dataset Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:32.000206900Z",
     "start_time": "2024-01-23T13:02:31.710762600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Korean['sentences'] = df_Korean['Translation'].apply(sent_tokenize)\n",
    "\n",
    "def group_sentences(sentences):\n",
    "    return [sentences[i:i+5] for i in range(0, len(sentences), 5)]\n",
    "\n",
    "df_Korean['grouped_sentences'] = df_Korean['sentences'].apply(group_sentences)\n",
    "\n",
    "new_df_Korean = df_Korean.explode('grouped_sentences')\n",
    "\n",
    "new_df_Korean[\"grouped_sentences\"] = new_df_Korean[\"grouped_sentences\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:32.232239Z",
     "start_time": "2024-01-23T13:02:32.172682400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.0954458685574\n"
     ]
    }
   ],
   "source": [
    "new_df_Korean[\"wc\"] = new_df_Korean[\"grouped_sentences\"].apply(word_count)\n",
    "print(np.mean(new_df_Korean[\"wc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:36.257703500Z",
     "start_time": "2024-01-23T13:02:36.236527400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "3667\n"
     ]
    }
   ],
   "source": [
    "print(len(df_Korean))\n",
    "print(len(new_df_Korean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:43.336040300Z",
     "start_time": "2024-01-23T13:02:43.320565100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "impact_length_idx\n0.0    1971\n1.0     695\n2.0    1001\nName: grouped_sentences, dtype: int64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_Korean.groupby(\"impact_length_idx\")[\"grouped_sentences\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T13:02:45.935038400Z",
     "start_time": "2024-01-23T13:02:44.481044600Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df_Korean.to_parquet(\"../dataset/train_df_Korean_translated_segmented.parquet\")\n",
    "new_df_Chinese.to_parquet(\"../dataset/train_df_Chinese_translated_segmented.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
