{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/xinyunrong/Desktop/code/ml-esg-3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load translated data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file has 2320 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>impact_length_idx</th>\n",
       "      <th>language</th>\n",
       "      <th>title_eng</th>\n",
       "      <th>content_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.esgtoday.com/arabesque-ai-appoints...</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>ESG-focused financial technology company Arabe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>ESG-focused financial technology company Arabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.esgtoday.com/arabesque-ai-appoints...</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>The company also announced the appointment of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>The company also announced the appointment of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.esgtoday.com/arabesque-ai-appoints...</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>Wong said:  “Personalised portfolios demand th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>Wong said:  “Personalised portfolios demand th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.esgtoday.com/ukraine-war-inflation...</td>\n",
       "      <td>Ukraine War, Inflation Reduction Act Driving F...</td>\n",
       "      <td>One of the key themes of the report is the imp...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Ukraine War, Inflation Reduction Act Driving F...</td>\n",
       "      <td>One of the key themes of the report is the imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.esgtoday.com/eu-regulators-welcome...</td>\n",
       "      <td>EU Regulators Welcome, Critique New European S...</td>\n",
       "      <td>Europe’s three primary financial regulatory ag...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>English</td>\n",
       "      <td>EU Regulators Welcome, Critique New European S...</td>\n",
       "      <td>Europe’s three primary financial regulatory ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.esgtoday.com/arabesque-ai-appoints...   \n",
       "1  https://www.esgtoday.com/arabesque-ai-appoints...   \n",
       "2  https://www.esgtoday.com/arabesque-ai-appoints...   \n",
       "3  https://www.esgtoday.com/ukraine-war-inflation...   \n",
       "4  https://www.esgtoday.com/eu-regulators-welcome...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "1  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "2  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "3  Ukraine War, Inflation Reduction Act Driving F...   \n",
       "4  EU Regulators Welcome, Critique New European S...   \n",
       "\n",
       "                                             content  impact_length_idx  \\\n",
       "0  ESG-focused financial technology company Arabe...                1.0   \n",
       "1  The company also announced the appointment of ...                1.0   \n",
       "2  Wong said:  “Personalised portfolios demand th...                1.0   \n",
       "3  One of the key themes of the report is the imp...                2.0   \n",
       "4  Europe’s three primary financial regulatory ag...                0.0   \n",
       "\n",
       "  language                                          title_eng  \\\n",
       "0  English  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "1  English  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "2  English  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "3  English  Ukraine War, Inflation Reduction Act Driving F...   \n",
       "4  English  EU Regulators Welcome, Critique New European S...   \n",
       "\n",
       "                                         content_eng  \n",
       "0  ESG-focused financial technology company Arabe...  \n",
       "1  The company also announced the appointment of ...  \n",
       "2  Wong said:  “Personalised portfolios demand th...  \n",
       "3  One of the key themes of the report is the imp...  \n",
       "4  Europe’s three primary financial regulatory ag...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trs = pd.read_parquet(\"../dataset/Translation_Dataset.parquet\")\n",
    "print(f\"This file has {len(df_trs)} samples.\")\n",
    "df_trs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_wc</th>\n",
       "      <th>content_wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2320.000000</td>\n",
       "      <td>2320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.267672</td>\n",
       "      <td>365.278017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.988230</td>\n",
       "      <td>405.737352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>3974.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title_wc   content_wc\n",
       "count  2320.000000  2320.000000\n",
       "mean     14.267672   365.278017\n",
       "std       5.988230   405.737352\n",
       "min       4.000000     8.000000\n",
       "25%      11.000000    74.000000\n",
       "50%      13.000000   125.000000\n",
       "75%      16.000000   577.000000\n",
       "max      48.000000  3974.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def word_count(article):\n",
    "    return len(regexp_tokenizer.tokenize(article))\n",
    "    \n",
    "\n",
    "df_trs[\"title_wc\"] = df_trs[\"title_eng\"].apply(word_count)\n",
    "df_trs[\"content_wc\"] = df_trs[\"content_eng\"].apply(word_count)\n",
    "\n",
    "df_trs[[\"title_wc\", \"content_wc\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count       mean       std  min   25%   50%   75%   max\n",
      "language                                                         \n",
      "Chinese   352.0  24.309659  7.916093  4.0  19.0  24.0  30.0  48.0\n",
      "English   545.0  11.400000  2.934581  5.0   9.0  11.0  13.0  21.0\n",
      "French    654.0  13.172783  3.088157  7.0  11.0  13.0  15.0  22.0\n",
      "Korean    769.0  12.634590  3.111705  4.0  11.0  12.0  15.0  25.0\n",
      "          count        mean         std    min    25%    50%     75%     max\n",
      "language                                                                    \n",
      "Chinese   352.0  925.994318  524.688593  173.0  564.5  764.0  1171.5  3974.0\n",
      "English   545.0   63.425688   27.222302    8.0   44.0   59.0    80.0   194.0\n",
      "French    654.0   84.678899   23.265120   16.0   69.0   84.0    98.0   225.0\n",
      "Korean    769.0  561.180754  236.733538  144.0  388.0  519.0   693.0  1455.0\n"
     ]
    }
   ],
   "source": [
    "# Korean and Chinese dataset are too long, indicating the needs to further segmenation\n",
    "\n",
    "print(df_trs.groupby(\"language\")[\"title_wc\"].describe())\n",
    "print(df_trs.groupby(\"language\")[\"content_wc\"].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Segment Chinese and Korean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Chinese and Korean articles into every n sentences as they are too long\n",
    "\n",
    "def group_sentences(sentences, sent_size):\n",
    "    return [sentences[i : i + sent_size] for i in range(0, len(sentences), sent_size)]\n",
    "\n",
    "def segment_articles(df, sent_size):\n",
    "    df['sent_tokenize'] = df['content_eng'].apply(sent_tokenize)\n",
    "    df['content_eng_short'] = df['sent_tokenize'].apply(lambda x: group_sentences(x, sent_size))\n",
    "    seg_df = df.explode('content_eng_short')\n",
    "    seg_df[\"content_eng_short\"] = seg_df[\"content_eng_short\"].apply(lambda x: \" \".join(x))\n",
    "    seg_df = seg_df.drop(columns=\"sent_tokenize\")\n",
    "    return seg_df\n",
    "\n",
    "# split the dataset to segment\n",
    "chn_kor_trs = df_trs[df_trs[\"language\"].isin([\"Korean\", \"Chinese\"])].copy()\n",
    "eng_fre_trs = df_trs[~df_trs[\"language\"].isin([\"Korean\", \"Chinese\"])].copy()\n",
    "    \n",
    "# segment Chinese and Korean articles\n",
    "sent_size = 5\n",
    "chn_kor_seg = segment_articles(chn_kor_trs, sent_size)\n",
    "chn_kor_seg[\"content_wc\"] = chn_kor_seg[\"content_eng_short\"].apply(word_count)\n",
    "\n",
    "# concatenate with English and French articles\n",
    "eng_fre_seg = eng_fre_trs\n",
    "eng_fre_seg[\"content_eng_short\"] = eng_fre_seg[\"content_eng\"]\n",
    "\n",
    "df_seg = pd.concat([chn_kor_seg, eng_fre_seg]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 1121 Korean & Chinese samples into 5447 samples.\n",
      "Expanding 2320 samples to 6646 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>1924.0</td>\n",
       "      <td>169.412682</td>\n",
       "      <td>64.362227</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>545.0</td>\n",
       "      <td>63.425688</td>\n",
       "      <td>27.222302</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>654.0</td>\n",
       "      <td>84.678899</td>\n",
       "      <td>23.265120</td>\n",
       "      <td>16.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>3523.0</td>\n",
       "      <td>122.494465</td>\n",
       "      <td>41.591435</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count        mean        std   min    25%    50%    75%    max\n",
       "language                                                                 \n",
       "Chinese   1924.0  169.412682  64.362227   2.0  130.0  170.0  207.0  425.0\n",
       "English    545.0   63.425688  27.222302   8.0   44.0   59.0   80.0  194.0\n",
       "French     654.0   84.678899  23.265120  16.0   69.0   84.0   98.0  225.0\n",
       "Korean    3523.0  122.494465  41.591435   2.0   99.0  124.0  150.0  336.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Segmenting {len(chn_kor_trs)} Korean & Chinese samples into {len(chn_kor_seg)} samples.\")\n",
    "print(f\"Expanding {len(df_trs)} samples to {len(df_seg)} samples.\")\n",
    "df_seg.groupby(\"language\")[\"content_wc\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Is the boss himself a punch card machine? Bill...</td>\n",
       "      <td>Date: 2022-01-04 work-life balance in the mode...</td>\n",
       "      <td>Is the boss himself a punch card machine? Bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Is the boss himself a punch card machine? Bill...</td>\n",
       "      <td>Microsoft's market capitalization will reach 1...</td>\n",
       "      <td>Is the boss himself a punch card machine? Bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Is the boss himself a punch card machine? Bill...</td>\n",
       "      <td>From Bill Gates' words, it can be inferred tha...</td>\n",
       "      <td>Is the boss himself a punch card machine? Bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Mercedes-Maybach is going electric too, with t...</td>\n",
       "      <td>Date: 2021-09-06Two years ago, IAA Mobility wa...</td>\n",
       "      <td>Mercedes-Maybach is going electric too, with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Mercedes-Maybach is going electric too, with t...</td>\n",
       "      <td>One is the new EQE, which is positioned under ...</td>\n",
       "      <td>Mercedes-Maybach is going electric too, with t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label language                                              title  \\\n",
       "0    2.0  Chinese  Is the boss himself a punch card machine? Bill...   \n",
       "1    2.0  Chinese  Is the boss himself a punch card machine? Bill...   \n",
       "2    2.0  Chinese  Is the boss himself a punch card machine? Bill...   \n",
       "3    0.0  Chinese  Mercedes-Maybach is going electric too, with t...   \n",
       "4    0.0  Chinese  Mercedes-Maybach is going electric too, with t...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Date: 2022-01-04 work-life balance in the mode...   \n",
       "1  Microsoft's market capitalization will reach 1...   \n",
       "2  From Bill Gates' words, it can be inferred tha...   \n",
       "3  Date: 2021-09-06Two years ago, IAA Mobility wa...   \n",
       "4  One is the new EQE, which is positioned under ...   \n",
       "\n",
       "                                             feature  \n",
       "0  Is the boss himself a punch card machine? Bill...  \n",
       "1  Is the boss himself a punch card machine? Bill...  \n",
       "2  Is the boss himself a punch card machine? Bill...  \n",
       "3  Mercedes-Maybach is going electric too, with t...  \n",
       "4  Mercedes-Maybach is going electric too, with t...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare dataset for data augmentation\n",
    "df_seg = df_seg.drop(columns=['url', 'title', 'content', 'content_eng', 'title_wc', 'content_wc'])\n",
    "\n",
    "df_seg = df_seg.rename(columns={\n",
    "    \"title_eng\": \"title\",\n",
    "    \"content_eng_short\": \"content\",\n",
    "    \"impact_length_idx\": \"label\"\n",
    "})\n",
    "\n",
    "df_seg[\"feature\"] = df_seg[\"title\"] + ' || ' + df_seg[\"content\"]\n",
    "\n",
    "df_seg.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Filter non-ESG related sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg',\n",
    "                                          truncation=True,\n",
    "                                          padding='max_length',\n",
    "                                          max_length=512)\n",
    "esg_pipeline = pipeline(\"text-classification\", \n",
    "                        model=finbert, \n",
    "                        tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "for i in range(0, len(df_seg) // batch + 1):\n",
    "    sentences = list(df_seg.loc[i * batch: (i + 1) * batch][\"content\"])\n",
    "    sentences = [s[:512] for s in sentences]\n",
    "    results = esg_pipeline(sentences)\n",
    "    df_seg.loc[i * batch: (i + 1) * batch, \"esg_label\"] = [x[\"label\"] for x in results]\n",
    "    df_seg.loc[i * batch: (i + 1) * batch, \"esg_score\"] = [x[\"score\"] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 531 samples that are not ESG related with over 0.9 probability.\n"
     ]
    }
   ],
   "source": [
    "# Filter out segmented Chinese and Korean paragraphs that are non ESG related.\n",
    "\n",
    "none_news = df_seg[(df_seg[\"esg_label\"] == \"None\") & (df_seg[\"esg_score\"] > 0.9) & (df_seg[\"language\"].isin([\"Chinese\", \"Korean\"]))]\n",
    "print(f\"There are {len(none_news)} samples that are not ESG related with over 0.9 probability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We filter down to 6115 samples.\n"
     ]
    }
   ],
   "source": [
    "df_seg = df_seg[~df_seg.index.isin(none_news.index)].reset_index(drop=True)\n",
    "print(f\"We filter down to {len(df_seg)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seg = df_seg.drop(columns=[\"esg_label\", \"esg_score\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    2143\n",
       "1.0    1354\n",
       "2.0    2618\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.groupby(\"label\")[\"label\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign group id for each title before train test split\n",
    "groups = list(df_seg[\"title\"].unique())\n",
    "group_indic_dict = {}\n",
    "for i, v in enumerate(groups):\n",
    "    group_indic_dict[v] = i\n",
    "    \n",
    "df_seg[\"group_indicator\"] = df_seg[\"title\"].map(group_indic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    562\n",
       "1.0    353\n",
       "2.0    656\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seg.groupby(\"group_indicator\").head(1).groupby(\"label\")[\"label\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data ensuring the same group is not in both train and test sets\n",
    "for train_idx, test_idx in gss.split(df_seg['feature'], \n",
    "                                     df_seg['label'], \n",
    "                                     df_seg['group_indicator']):\n",
    "    train_set = df_seg.iloc[train_idx]\n",
    "    valid_set = df_seg.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    1681\n",
      "1.0    1092\n",
      "2.0    2104\n",
      "Name: label, dtype: int64\n",
      "label\n",
      "0.0    462\n",
      "1.0    262\n",
      "2.0    514\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.groupby(\"label\")[\"label\"].count())\n",
    "print(valid_set.groupby(\"label\")[\"label\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "Chinese    1421\n",
      "English     433\n",
      "French      543\n",
      "Korean     2480\n",
      "Name: label, dtype: int64\n",
      "language\n",
      "Chinese    341\n",
      "English    112\n",
      "French     111\n",
      "Korean     674\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.groupby(\"language\")[\"label\"].count())\n",
    "print(valid_set.groupby(\"language\")[\"label\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_parquet(\"../dataset/training_dataset.parquet\")\n",
    "valid_set.to_parquet(\"../dataset/validation_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
