{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/xinyunrong/Desktop/code/ml-esg-3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load translated data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file has 2320 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>impact_length_idx</th>\n",
       "      <th>language</th>\n",
       "      <th>title_eng</th>\n",
       "      <th>content_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.esgtoday.com/arabesque-ai-appoints...</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>ESG-focused financial technology company Arabe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>ESG-focused financial technology company Arabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.esgtoday.com/arabesque-ai-appoints...</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>The company also announced the appointment of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>The company also announced the appointment of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.esgtoday.com/arabesque-ai-appoints...</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>Wong said:  “Personalised portfolios demand th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Arabesque AI Appoints Carolina Minio Paluello ...</td>\n",
       "      <td>Wong said:  “Personalised portfolios demand th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.esgtoday.com/ukraine-war-inflation...</td>\n",
       "      <td>Ukraine War, Inflation Reduction Act Driving F...</td>\n",
       "      <td>One of the key themes of the report is the imp...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>English</td>\n",
       "      <td>Ukraine War, Inflation Reduction Act Driving F...</td>\n",
       "      <td>One of the key themes of the report is the imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.esgtoday.com/eu-regulators-welcome...</td>\n",
       "      <td>EU Regulators Welcome, Critique New European S...</td>\n",
       "      <td>Europe’s three primary financial regulatory ag...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>English</td>\n",
       "      <td>EU Regulators Welcome, Critique New European S...</td>\n",
       "      <td>Europe’s three primary financial regulatory ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.esgtoday.com/arabesque-ai-appoints...   \n",
       "1  https://www.esgtoday.com/arabesque-ai-appoints...   \n",
       "2  https://www.esgtoday.com/arabesque-ai-appoints...   \n",
       "3  https://www.esgtoday.com/ukraine-war-inflation...   \n",
       "4  https://www.esgtoday.com/eu-regulators-welcome...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "1  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "2  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "3  Ukraine War, Inflation Reduction Act Driving F...   \n",
       "4  EU Regulators Welcome, Critique New European S...   \n",
       "\n",
       "                                             content  impact_length_idx  \\\n",
       "0  ESG-focused financial technology company Arabe...                1.0   \n",
       "1  The company also announced the appointment of ...                1.0   \n",
       "2  Wong said:  “Personalised portfolios demand th...                1.0   \n",
       "3  One of the key themes of the report is the imp...                2.0   \n",
       "4  Europe’s three primary financial regulatory ag...                0.0   \n",
       "\n",
       "  language                                          title_eng  \\\n",
       "0  English  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "1  English  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "2  English  Arabesque AI Appoints Carolina Minio Paluello ...   \n",
       "3  English  Ukraine War, Inflation Reduction Act Driving F...   \n",
       "4  English  EU Regulators Welcome, Critique New European S...   \n",
       "\n",
       "                                         content_eng  \n",
       "0  ESG-focused financial technology company Arabe...  \n",
       "1  The company also announced the appointment of ...  \n",
       "2  Wong said:  “Personalised portfolios demand th...  \n",
       "3  One of the key themes of the report is the imp...  \n",
       "4  Europe’s three primary financial regulatory ag...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trs = pd.read_parquet(\"../dataset/Translation_Dataset.parquet\")\n",
    "print(f\"This file has {len(df_trs)} samples.\")\n",
    "df_trs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_wc</th>\n",
       "      <th>content_wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2320.000000</td>\n",
       "      <td>2320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.267672</td>\n",
       "      <td>365.278017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.988230</td>\n",
       "      <td>405.737352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>3974.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title_wc   content_wc\n",
       "count  2320.000000  2320.000000\n",
       "mean     14.267672   365.278017\n",
       "std       5.988230   405.737352\n",
       "min       4.000000     8.000000\n",
       "25%      11.000000    74.000000\n",
       "50%      13.000000   125.000000\n",
       "75%      16.000000   577.000000\n",
       "max      48.000000  3974.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def word_count(article):\n",
    "    return len(regexp_tokenizer.tokenize(article))\n",
    "    \n",
    "\n",
    "df_trs[\"title_wc\"] = df_trs[\"title_eng\"].apply(word_count)\n",
    "df_trs[\"content_wc\"] = df_trs[\"content_eng\"].apply(word_count)\n",
    "\n",
    "df_trs[[\"title_wc\", \"content_wc\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count       mean       std  min   25%   50%   75%   max\n",
      "language                                                         \n",
      "Chinese   352.0  24.309659  7.916093  4.0  19.0  24.0  30.0  48.0\n",
      "English   545.0  11.400000  2.934581  5.0   9.0  11.0  13.0  21.0\n",
      "French    654.0  13.172783  3.088157  7.0  11.0  13.0  15.0  22.0\n",
      "Korean    769.0  12.634590  3.111705  4.0  11.0  12.0  15.0  25.0\n",
      "          count        mean         std    min    25%    50%     75%     max\n",
      "language                                                                    \n",
      "Chinese   352.0  925.994318  524.688593  173.0  564.5  764.0  1171.5  3974.0\n",
      "English   545.0   63.425688   27.222302    8.0   44.0   59.0    80.0   194.0\n",
      "French    654.0   84.678899   23.265120   16.0   69.0   84.0    98.0   225.0\n",
      "Korean    769.0  561.180754  236.733538  144.0  388.0  519.0   693.0  1455.0\n"
     ]
    }
   ],
   "source": [
    "# Korean and Chinese dataset are too long, indicating the needs to further segmenation\n",
    "\n",
    "print(df_trs.groupby(\"language\")[\"title_wc\"].describe())\n",
    "print(df_trs.groupby(\"language\")[\"content_wc\"].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Segment Chinese and Korean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Chinese and Korean articles into every n sentences as they are too long\n",
    "\n",
    "def group_sentences(sentences, sent_size):\n",
    "    return [sentences[i : i + sent_size] for i in range(0, len(sentences), sent_size)]\n",
    "\n",
    "def segment_articles(df, sent_size):\n",
    "    df['sent_tokenize'] = df['content_eng'].apply(sent_tokenize)\n",
    "    df['content_eng_short'] = df['sent_tokenize'].apply(lambda x: group_sentences(x, sent_size))\n",
    "    seg_df = df.explode('content_eng_short')\n",
    "    seg_df[\"content_eng_short\"] = seg_df[\"content_eng_short\"].apply(lambda x: \" \".join(x))\n",
    "    seg_df = seg_df.drop(columns=\"sent_tokenize\")\n",
    "    return seg_df\n",
    "\n",
    "# split the dataset to segment\n",
    "chn_kor_trs = df_trs[df_trs[\"language\"].isin([\"Korean\", \"Chinese\"])].copy()\n",
    "eng_fre_trs = df_trs[~df_trs[\"language\"].isin([\"Korean\", \"Chinese\"])].copy()\n",
    "    \n",
    "# segment Chinese and Korean articles\n",
    "sent_size = 5\n",
    "chn_kor_seg = segment_articles(chn_kor_trs, sent_size)\n",
    "\n",
    "# concatenate with English and French articles\n",
    "eng_fre_seg = eng_fre_trs\n",
    "eng_fre_seg[\"content_eng_short\"] = eng_fre_seg[\"content_eng\"]\n",
    "\n",
    "df_seg = pd.concat([chn_kor_seg, eng_fre_seg]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 1121 samples into 5381 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>1900.0</td>\n",
       "      <td>171.140000</td>\n",
       "      <td>64.654063</td>\n",
       "      <td>7.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>3481.0</td>\n",
       "      <td>123.406492</td>\n",
       "      <td>41.848859</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count        mean        std  min    25%    50%    75%    max\n",
       "language                                                                \n",
       "Chinese   1900.0  171.140000  64.654063  7.0  131.0  171.0  208.0  425.0\n",
       "Korean    3481.0  123.406492  41.848859  6.0  100.0  125.0  150.0  336.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Segmenting {len(chn_kor_trs)} samples into {len(chn_kor_seg)} samples.\")\n",
    "chn_kor_seg[\"content_wc\"] = chn_kor_seg[\"content_eng_short\"].apply(word_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English and French articles are already segmented\n",
    "eng_seg = eng_trs\n",
    "eng_seg[\"content_eng_short\"] = eng_seg[\"content_eng\"]\n",
    "fre_seg = fre_trs\n",
    "fre_seg[\"content_eng_short\"] = fre_seg[\"content_eng\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Aggregate Segmented Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"title_eng\", \"content_eng_short\", \"impact_length_idx\", \"language\"]\n",
    "\n",
    "eng_seg = eng_seg[columns]\n",
    "fre_seg = fre_seg[columns]\n",
    "kor_seg = kor_seg[columns]\n",
    "chn_seg = chn_seg[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([eng_seg, fre_seg, kor_seg, chn_seg]).reset_index(drop=True)\n",
    "print(f\"Train dataset for short news article has {len(train_df)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns={\n",
    "    \"title_eng\": \"title\",\n",
    "    \"content_eng_short\": \"content\",\n",
    "    \"impact_length_idx\": \"label\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the segmented articles length stats\n",
    "train_df[\"content\"].apply(word_count).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Augment the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
