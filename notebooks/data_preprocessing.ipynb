{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_English = pd.read_parquet(\"../dataset/train_df_English_translated.parquet\")\n",
    "df_French = pd.read_parquet(\"../dataset/train_df_French_translated.parquet\")\n",
    "df_Korean = pd.read_parquet(\"../dataset/train_df_Korean_translated.parquet\")\n",
    "df_Chinese = pd.read_parquet(\"../dataset/train_df_Chinese_translated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"sentence\", \"Translation\", \"impact_length_idx\", \"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_English[\"Translation\"] = df_English[\"sentence\"]\n",
    "df_English = df_English[columns]\n",
    "df_Chinese = df_Chinese[columns]\n",
    "df_Korean = df_Korean[columns]\n",
    "df_French = df_French[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_English, df_Chinese, df_Korean, df_French]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"../dataset/train_df_all_english.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    return len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"wc\"] = df[\"Translation\"].map(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/36tqpxqn7c7645qp2sw_pwwh0000gn/T/ipykernel_32447/1805526242.py:1: FutureWarning: The provided callable <function mean at 0x107fe0ca0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  df.groupby(\"language\").agg({\"wc\": np.mean})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>903.184659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>73.864220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>96.636914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>555.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  wc\n",
       "language            \n",
       "Chinese   903.184659\n",
       "English    73.864220\n",
       "French     96.636914\n",
       "Korean    555.970000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"language\").agg({\"wc\": np.mean})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Chinese text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter very short sentences and non-sentences (defined by no . at the end of the sentence)\n",
    "def Chinese_article_preprocess(article):\n",
    "    sentences = article.split(\"\\n\")\n",
    "    filtered_sentences = [text for text in sentences if len(text.split(\" \")) > 5 ]\n",
    "    title = filtered_sentences[0]\n",
    "    filtered_sentences = filtered_sentences[1:]\n",
    "    filtered_sentences = [text for text in filtered_sentences if text[-1] == \".\"]\n",
    "    article = title + \" \" + \" \".join(filtered_sentences)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Chinese[\"processed\"] = df_Chinese[\"Translation\"].map(Chinese_article_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.0795454545455\n"
     ]
    }
   ],
   "source": [
    "# we reduced the average word count from 900 to 750.\n",
    "df_Chinese[\"wc\"] = df_Chinese[\"processed\"].map(word_count)\n",
    "print(np.mean(df_Chinese[\"wc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Chinese['sentences'] = df_Chinese['processed'].apply(sent_tokenize)\n",
    "\n",
    "def group_sentences(sentences):\n",
    "    return [sentences[i:i+5] for i in range(0, len(sentences), 5)]\n",
    "\n",
    "df_Chinese['grouped_sentences'] = df_Chinese['sentences'].apply(group_sentences)\n",
    "\n",
    "new_df_Chinese = df_Chinese.explode('grouped_sentences')\n",
    "\n",
    "new_df_Chinese[\"grouped_sentences\"] = new_df_Chinese[\"grouped_sentences\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "1717\n"
     ]
    }
   ],
   "source": [
    "print(len(df_Chinese))\n",
    "print(len(new_df_Chinese))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.57483983692487\n"
     ]
    }
   ],
   "source": [
    "new_df_Chinese[\"wc\"] = new_df_Chinese[\"grouped_sentences\"].apply(word_count)\n",
    "print(np.mean(new_df_Chinese[\"wc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "impact_length_idx\n",
       "0.0     337\n",
       "1.0     315\n",
       "2.0    1065\n",
       "Name: grouped_sentences, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_Chinese.groupby(\"impact_length_idx\")[\"grouped_sentences\"].count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Korean['sentences'] = df_Korean['Translation'].apply(sent_tokenize)\n",
    "\n",
    "def group_sentences(sentences):\n",
    "    return [sentences[i:i+5] for i in range(0, len(sentences), 5)]\n",
    "\n",
    "df_Korean['grouped_sentences'] = df_Korean['sentences'].apply(group_sentences)\n",
    "\n",
    "new_df_Korean = df_Korean.explode('grouped_sentences')\n",
    "\n",
    "new_df_Korean[\"grouped_sentences\"] = new_df_Korean[\"grouped_sentences\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.0954458685574\n"
     ]
    }
   ],
   "source": [
    "new_df_Korean[\"wc\"] = new_df_Korean[\"grouped_sentences\"].apply(word_count)\n",
    "print(np.mean(new_df_Korean[\"wc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "3667\n"
     ]
    }
   ],
   "source": [
    "print(len(df_Korean))\n",
    "print(len(new_df_Korean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "impact_length_idx\n",
       "0.0    1971\n",
       "1.0     695\n",
       "2.0    1001\n",
       "Name: grouped_sentences, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_Korean.groupby(\"impact_length_idx\")[\"grouped_sentences\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
