{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/xinyunrong/Desktop/code/ml-esg-3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from src.constant import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load translated data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_trs = pd.read_parquet(\"../dataset/train_df_English_translated.parquet\")\n",
    "fre_trs = pd.read_parquet(\"../dataset/train_df_French_translated.parquet\")\n",
    "kor_trs = pd.read_parquet(\"../dataset/train_df_Korean_translated.parquet\")\n",
    "chn_trs = pd.read_parquet(\"../dataset/train_df_Chinese_translated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"url\", \"title\", \"content\", \"impact_length_idx\", \"language\", \"title_eng\", \"content_eng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_trs = eng_trs.drop_duplicates()\n",
    "fre_trs = fre_trs.drop_duplicates() # from 661 to 654\n",
    "kor_trs = kor_trs.drop_duplicates() # from 800 to 771\n",
    "chn_trs = chn_trs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_trs[\"Translation\"] = eng_trs[\"sentence\"] # English dataset doesn't need translation\n",
    "\n",
    "eng_trs[\"title_eng\"] = eng_trs[\"title\"]\n",
    "eng_trs[\"content_eng\"] = eng_trs[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter very short sentences and non-sentences (defined by no . at the end of the sentence)\n",
    "def Chinese_article_preprocess(article):\n",
    "    sentences = article.split(\"\\n\")\n",
    "    filtered_sentences = [text for text in sentences if len(text.split(\" \")) > 5 ]\n",
    "    title = filtered_sentences[0]\n",
    "    filtered_sentences = filtered_sentences[1:]\n",
    "    filtered_sentences = [text for text in filtered_sentences if text[-1] == \".\"]\n",
    "    article = title + \" \" + \" \".join(filtered_sentences)\n",
    "    return article\n",
    "\n",
    "chn_trs[\"Translation\"] = chn_trs[\"Translation\"].map(Chinese_article_preprocess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Segment Chinese and Korean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Chinese and Korean articles into every n sentences as they are too long\n",
    "\n",
    "def group_sentences(sentences, sent_size):\n",
    "    return [sentences[i : i + sent_size] for i in range(0, len(sentences), sent_size)]\n",
    "\n",
    "def segment_articles(df, sent_size):\n",
    "    df['sent_tokenize'] = df['content_eng'].apply(sent_tokenize)\n",
    "    df['content_eng_short'] = df['sent_tokenize'].apply(group_sentences, sent_size)\n",
    "    seg_df = df.explode('content_eng_short')\n",
    "    seg_df[\"content_eng_short\"] = seg_df[\"content_eng_short\"].apply(lambda x: \" \".join(x))\n",
    "    seg_df = seg_df.drop(columns=\"sent_tokenize\")\n",
    "    return seg_df\n",
    "    \n",
    "sent_size = 5\n",
    "chn_seg = segment_articles(chn_trs, sent_size)\n",
    "kor_seg = segment_articles(kor_trs, sent_size)\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split(\" \"))\n",
    "\n",
    "# filter short sentences\n",
    "chn_seg = chn_seg[chn_seg[\"content_eng_short\"].apply(word_count) > 30]\n",
    "kor_seg = kor_seg[kor_seg[\"content_eng_short\"].apply(word_count) > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English and French articles are already segmented\n",
    "eng_seg = eng_trs\n",
    "eng_seg[\"content_eng_short\"] = eng_seg[\"content_eng\"]\n",
    "fre_seg = fre_trs\n",
    "fre_seg[\"content_eng_short\"] = fre_seg[\"content_eng\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Aggregate Segmented Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"title_eng\", \"content_eng_short\", \"impact_length_idx\", \"language\"]\n",
    "\n",
    "eng_seg = eng_seg[columns]\n",
    "fre_seg = fre_seg[columns]\n",
    "kor_seg = kor_seg[columns]\n",
    "chn_seg = chn_seg[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([eng_seg, fre_seg, kor_seg, chn_seg]).reset_index(drop=True)\n",
    "print(f\"Train dataset for short news article has {len(train_df)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns={\n",
    "    \"title_eng\": \"title\",\n",
    "    \"content_eng_short\": \"content\",\n",
    "    \"impact_length_idx\": \"label\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the segmented articles length stats\n",
    "train_df[\"content\"].apply(word_count).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Augment the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
