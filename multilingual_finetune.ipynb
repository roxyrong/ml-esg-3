{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from random import choices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 618 µs (started: 2024-01-05 17:17:17 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 150 µs (started: 2024-01-05 17:17:17 +08:00)\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" # change to \"cuda\" if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.99 s (started: 2024-01-05 17:17:17 +08:00)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', token=HF_TOKEN)\n",
    "pretrained = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", token=HF_TOKEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForSequenceClassification                           --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              91,812,096\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─Embedding: 3-3                              1,536\n",
       "│    │    └─LayerNorm: 3-4                              1,536\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             85,054,464\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 590,592\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           1,538\n",
       "================================================================================\n",
       "Total params: 177,854,978\n",
       "Trainable params: 177,854,978\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.3 ms (started: 2024-01-05 17:17:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "summary(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 408 µs (started: 2024-01-05 17:17:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# for param in pretrained.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "for layer in pretrained.bert.encoder.layer[:6]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForSequenceClassification                           --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              91,812,096\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─Embedding: 3-3                              1,536\n",
       "│    │    └─LayerNorm: 3-4                              1,536\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             85,054,464\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 590,592\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           1,538\n",
       "================================================================================\n",
       "Total params: 177,854,978\n",
       "Trainable params: 135,327,746\n",
       "Non-trainable params: 42,527,232\n",
       "================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.9 ms (started: 2024-01-05 17:17:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "summary(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 73.6 ms (started: 2024-01-05 17:17:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_parquet(\"dataset/train_df.parquet\")\n",
    "data_df = data_df[data_df[\"language\"].isin([\"English\", \"Korean\"])]\n",
    "data_df[\"sentence\"] = data_df[\"title\"] + \" || \" + data_df[\"content\"]\n",
    "data_df[\"stratified_col\"] = data_df[\"impact_length_idx\"].astype(str) + data_df[\"language\"] # for train/test split\n",
    "data_df[\"resample_col\"] = data_df[\"stratified_col\"] # for resampling\n",
    "esg_dataset = Dataset.from_pandas(data_df, preserve_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6403474ceb5d4237aa0a95316de12194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/1345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c320e1003865461fb99a18374ff1e977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72db4d10d03a4037884ed28b54911682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 73 ms (started: 2024-01-05 17:17:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "esg_dataset = esg_dataset.class_encode_column(\"impact_length_idx\") # encode label\n",
    "train_valid = esg_dataset.class_encode_column(\"stratified_col\").train_test_split(test_size=0.25, \n",
    "                                                                                 stratify_by_column=\"stratified_col\") # encode for splits\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cf3ce035a3406d851f5090b0b313df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8f7503d73f4d029c3c6036840189d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c6aed5e64744e1b6453b395b572cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a372d326073443dabe12b1422459e394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e34cfb5e92d413dbfd9a8ea1fc41c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7036e12f5a984311accd68eef505d80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 231 ms (started: 2024-01-05 17:17:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "train_idx = sorted(train_dataset['__index_level_0__'])\n",
    "class_counts = data_df.loc[train_idx]['resample_col'].value_counts()\n",
    "majority_class = class_counts.idxmax()\n",
    "majority_count = class_counts.max()\n",
    "\n",
    "def upsample(dataset, majority_class, majority_count):\n",
    "    # upsample every class to the same size of majority class\n",
    "    \n",
    "    upsampled_datasets = []\n",
    "\n",
    "    for class_label in set(dataset['resample_col']):\n",
    "        class_dataset = dataset.filter(lambda x: x[\"resample_col\"] == class_label)\n",
    "        \n",
    "        if majority_class == class_label:   \n",
    "            upsampled_datasets.append(class_dataset)\n",
    "        else:\n",
    "            upsampled = class_dataset.select(choices(range(len(class_dataset)), k=majority_count))\n",
    "            upsampled_datasets.append(upsampled)\n",
    "            \n",
    "    return concatenate_datasets(upsampled_datasets)\n",
    "\n",
    "resampled_train = upsample(train_dataset, majority_class,majority_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 858 µs (started: 2024-01-05 17:17:21 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    sentences = [i[\"sentence\"] for i in data]\n",
    "    labels = torch.tensor([i[\"impact_length_idx\"] for i in data]).to(device)\n",
    "\n",
    "    data = tokenizer.batch_encode_plus(sentences,\n",
    "                                       truncation=True,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=400,\n",
    "                                       return_tensors='pt',\n",
    "                                       return_length=True)\n",
    "\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    token_type_ids = data['token_type_ids'].to(device)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=resampled_train,\n",
    "                                    batch_size=16,\n",
    "                                    collate_fn=collate_fn,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=True)\n",
    "\n",
    "loader_valid = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                            batch_size=16,\n",
    "                                            collate_fn=collate_fn,\n",
    "                                            shuffle=True,\n",
    "                                            drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Linear                                   2,307\n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "time: 7.12 ms (started: 2024-01-05 17:17:21 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 3).to(device)\n",
    "        print(summary(self.fc))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        out = pretrained(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    output_hidden_states=True)\n",
    "\n",
    "        out = self.fc(out[\"hidden_states\"][-1][:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/3\n",
      "Epoch 1/3, Step 0, Loss: 1.0950053930282593, Accuracy: 0.375\n",
      "Epoch 1/3, Step 25, Loss: 1.1123788356781006, Accuracy: 0.3125\n",
      "Epoch 1/3, Step 50, Loss: 1.1140973567962646, Accuracy: 0.3125\n",
      "Epoch 1/3, Step 75, Loss: 1.0939059257507324, Accuracy: 0.3125\n",
      "Epoch 1/3, Step 100, Loss: 1.1015647649765015, Accuracy: 0.3125\n",
      "epoch valid accuracy: 0.5\n",
      "Starting epoch 2/3\n",
      "Epoch 2/3, Step 0, Loss: 1.1164498329162598, Accuracy: 0.25\n",
      "Epoch 2/3, Step 25, Loss: 1.1102522611618042, Accuracy: 0.1875\n",
      "Epoch 2/3, Step 50, Loss: 1.1159625053405762, Accuracy: 0.125\n",
      "Epoch 2/3, Step 75, Loss: 1.0914514064788818, Accuracy: 0.375\n",
      "Epoch 2/3, Step 100, Loss: 1.0961025953292847, Accuracy: 0.375\n",
      "epoch valid accuracy: 0.3125\n",
      "Starting epoch 3/3\n",
      "Epoch 3/3, Step 0, Loss: 1.0937806367874146, Accuracy: 0.375\n",
      "Epoch 3/3, Step 25, Loss: 1.0982227325439453, Accuracy: 0.3125\n",
      "Epoch 3/3, Step 50, Loss: 1.1068987846374512, Accuracy: 0.1875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m torch\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m     31\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, labels)\n\u001b[0;32m---> 32\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     34\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m     35\u001b[0m torch\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3h 45min 58s (started: 2024-01-05 17:17:21 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#5e^4, 0.01\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=2e-5, \n",
    "                              weight_decay=0.01)\n",
    "\n",
    "# Total number of training steps\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "# Scheduler including warm-up\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_training_steps * 0.1, \n",
    "                                            num_training_steps=num_training_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "        if i % 25 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {i}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "\n",
    "    out = out.argmax(dim=1)\n",
    "    correct += (out == labels).sum().item()\n",
    "    total += len(labels)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(\"epoch valid accuracy:\", accuracy)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "        \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_valid):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(out.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"0\", linewidths=.5,\n",
    "                square = True, cmap = \"Blues\")\n",
    "    plt.ylabel(\"Actual label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.xticks(ticks=np.arange(3) + 0.5, labels=[0, 1, 2], rotation=45, ha='right')\n",
    "    plt.yticks(ticks=np.arange(3) + 0.5, labels=[0, 1, 2], rotation=0)\n",
    "\n",
    "    all_sample_title = \"Confusion Matrix\"\n",
    "    plt.title(all_sample_title, size = 15)\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
