seed: 3407
pretrained_model: nbroad/ESG-BERT
tokenizer_name: nbroad/ESG-BERT
device: mps

datamodule:
  train_path: dataset/training_dataset.parquet
  augment: False
  text_col: feature
  label_col: label
  stratify_col: label
  max_length: 512
  use_cls_weight: True
  batch_size: 8
  test_size: 0.2

model:
  hidden_size: 768
  layer_sizes: [256, 128, 64, 32]
  num_labels: 3
  dropout: 0.15
  activation: LeakyReLU

trainer:
  num_epochs: 20
  lr: 1e-04
  weight_decay: 0.01
  warm_up_step: 0.1
